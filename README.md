# Airflow-CICD

This repository implements a **CI/CD pipeline** for orchestrating **flight booking data processing** using **Apache Airflow (Composer)**, **Google Cloud Dataproc Serverless**, and **BigQuery**.  

The pipeline automates:
- Uploading environment variables to Cloud Composer.
- Deploying Airflow DAGs to Composer environments.
- Deploying PySpark transformation jobs to GCS.
- Running data transformations and loading results into BigQuery.

---

## ğŸ“‚ Repository Structure

â”œâ”€â”€ .github/workflows/ci-cd.yml # GitHub Actions workflow (CI/CD pipeline)
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ airflow_job/airflow_job.py # Airflow DAG definition
â”‚ â””â”€â”€ spark_job/spark_job.py # PySpark transformation script
â”œâ”€â”€ variables/
â”‚ â”œâ”€â”€ dev/variables.json # Airflow variables for DEV environment
â”‚ â””â”€â”€ prod/variables.json # Airflow variables for PROD environment

---

## âš™ï¸ CI/CD Workflow

The GitHub Actions workflow (`ci-cd.yml`) is triggered on pushes to `dev` and `main` branches:

### ğŸ”¹ Dev Deployment (`dev` branch)
1. **Upload Variables JSON** â†’ to Composer dev bucket.  
2. **Import Variables into Airflow-DEV** â†’ using `gcloud composer environments run`.  
3. **Upload Spark Job** â†’ to GCS path:  
   `gs://airflows-projects-buckets/flight-booking-analysis/spark-job/`.  
4. **Upload Airflow DAG** â†’ to Composer DEV DAG bucket.  

### ğŸ”¹ Prod Deployment (`main` branch)
1. **Upload Variables JSON** â†’ to Composer prod bucket.  
2. **Import Variables into Airflow-PROD**.  
3. **Upload Spark Job** â†’ to GCS.  
4. **Upload Airflow DAG** â†’ to Composer PROD DAG bucket.  

---

## ğŸš€ Airflow DAG

**File:** [`airflow_job.py`](src/airflow_job/airflow_job.py)  

The DAG:
- Waits for a **source CSV file** in GCS (`flight_booking.csv`).  
- Submits a **PySpark job** to **Dataproc Serverless**.  
- Transforms the data and loads results into **BigQuery**.  

DAG ID: `flight_booking_dataproc_bq_dag`

---

## ğŸ”¥ PySpark Job

**File:** [`spark_job.py`](src/spark_job/spark_job.py)  

Steps performed:
1. Reads raw flight booking CSV from GCS.  
2. Performs transformations:  
   - Weekend flag  
   - Lead time categorization  
   - Booking success rate calculation  
3. Aggregates insights:  
   - Route-level insights  
   - Booking origin insights  
4. Writes results into BigQuery tables (overwrite mode).  

Arguments:
- `--env` â†’ environment (`dev`, `prod`)  
- `--bq_project` â†’ BigQuery project ID  
- `--bq_dataset` â†’ BigQuery dataset name  
- `--transformed_table` â†’ table for transformed data  
- `--route_insights_table` â†’ table for route insights  
- `--origin_insights_table` â†’ table for origin insights  

---

## ğŸ”‘ Requirements

- **Google Cloud Composer** (Airflow 2.x)  
- **Dataproc Serverless** (PySpark)  
- **BigQuery** enabled  
- GitHub secrets:
  - `GCP_SA_KEY` â†’ Service account JSON key  
  - `GCP_PROJECT_ID` â†’ GCP Project ID  

---

## â–¶ï¸ Running Locally

You can test the PySpark job locally (with Spark installed):

```bash
spark-submit src/spark_job/spark_job.py \
  --env dev \
  --bq_project my-gcp-project \
  --bq_dataset flight_data_dev \
  --transformed_table transformed_bookings \
  --route_insights_table route_insights \
  --origin_insights_table origin_insights
